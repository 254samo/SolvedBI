{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DATA602_Proj_ImplementingMLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM5b3FuLpK3noqa/dkO74w6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "COGO5yPjhugx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcGuMX6JBjst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime as dt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score,log_loss,accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm,tqdm_notebook,_tqdm_notebook\n",
        "tqdm.pandas()\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() "
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8fS5FZspekj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Prepping the data\n",
        "url='https://raw.githubusercontent.com/254samo/SolvedBI/master/On_Time_Performance.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df = df[df['Facility']=='JFK']\n",
        "df = df.reset_index()\n",
        "del df['index']\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "#df.head()\n",
        "\n",
        "df['month'] = df['Date'].dt.month\n",
        "df['weekday'] = df['Date'].dt.day_name()\n",
        "#df['weekday'] = df['Date'].dt.weekday_name\n",
        "#df.head()\n",
        "\n",
        "df['80% or More On Time'] = 0\n",
        "df.loc[df['% On-Time Gate Arrivals'] >= 80, '80% or More On Time'] = 1\n",
        "\n",
        "df['month'] = df['month'].apply(str)\n",
        "#df = pd.get_dummies(df, drop_first=True)\n",
        "df = df.drop(['Facility'], axis=1)\n",
        "#df.head()"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Q3a57i4M7y",
        "colab_type": "text"
      },
      "source": [
        "Extracting the Peak Hours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ6mZf0p4G8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extracting the Peak Hours \n",
        "peakHours = df.loc[((df['Hour']>=6) & (df['Hour']<= 9)) | ((df['Hour'] >= 15) & (df['Hour']<= 18)) | ((df['Hour']>= 20)) | ((df['Hour']<= 1))]\n",
        "#peakHours['Hour'] = peakHours['Hour'].apply(str)\n",
        "#peakHours = pd.get_dummies(peakHours, drop_first=True)\n",
        "#peakHours = peakHours.drop(['Average Gate Arrival Delay', 'Scheduled Departures', 'Scheduled Arrivals', 'Date', '% On-Time Gate Arrivals', 'Average Gate Departure Delay'], axis=1)\n",
        "peakHours = peakHours.drop(['Average Gate Arrival Delay', 'Scheduled Departures', 'Scheduled Arrivals', 'Date', '% On-Time Gate Arrivals', 'Average Gate Departure Delay', 'Hour', 'month', 'weekday'], axis=1)\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG6K6NaFkREB",
        "colab_type": "text"
      },
      "source": [
        "Extracting the Off-Peak Hours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLtrDtQPs2cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extracting the Off-Peak Hours \n",
        "offPeak = df.loc[((df['Hour']>=2) & (df['Hour']<= 5)) | ((df['Hour'] >= 10) & (df['Hour']<= 14)) | ((df['Hour']== 19))]\n",
        "#offPeak['Hour'] = offPeak['Hour'].apply(str)\n",
        "#offPeak = pd.get_dummies(offPeak, drop_first=True)\n",
        "#offPeak = offPeak.drop(['Average Gate Arrival Delay', 'Scheduled Departures', 'Scheduled Arrivals', 'Date', '% On-Time Gate Arrivals', 'Average Gate Departure Delay'], axis=1)\n",
        "offPeak = offPeak.drop(['Average Gate Arrival Delay', 'Scheduled Departures', 'Scheduled Arrivals', 'Date', '% On-Time Gate Arrivals', 'Average Gate Departure Delay', 'Hour', 'month', 'weekday'], axis=1)\n",
        "\n",
        "#offPeak.head(10)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG14BnBOSalb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "the_class_column = '80% or More On Time'"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h7N9sl-mzOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def perceptron_prep_and_implementation(filename):\n",
        "    data = filename\n",
        "    \n",
        "    \n",
        "    #Suppressing warnings and infos of TensorFlow\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "    SEED = 1000 #Fixing seed\n",
        "    synthetic = False# whether to use synthetic data\n",
        "\n",
        "    def seed_everything(seed=SEED):\n",
        "\t    random.seed(seed)\n",
        "\t    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\t    np.random.seed(seed)\n",
        "\t    tf.set_random_seed(seed) #tf.random.set_seed(seed)\n",
        "    seed_everything(seed=SEED)\n",
        "\n",
        "\t# THIS MARKS THE BEGINNING OF OUR PERCEPTION PREP\n",
        "\t#We assign the dataframe to be processed to the dataframe name that is used in the perceptron\n",
        "\t\n",
        "\t\n",
        "\n",
        "    # HyperParameters\n",
        "    lr = 0.0003\n",
        "    training_epochs = 50\n",
        "    batch_size = 4\n",
        "    pred_size = int(batch_size * 2)\n",
        "\n",
        "\n",
        "\t\n",
        "\t\n",
        "\t#Shuffling data and resetting index\n",
        "    data = shuffle(data)\n",
        "    data = data.reset_index(drop=True)\n",
        "\t\n",
        "\t#Now since data preparation is done, we assign the index to the class_column variable to a parameter\n",
        "    the_class_column_index = data.columns.get_loc(the_class_column)\n",
        "\n",
        "    # Get the Count of Rows and Columns in data\n",
        "    total_rows_in_data = int(len(data.axes[0]))\n",
        "    total_cols_in_data = int(len(data.axes[1]))\n",
        "\n",
        "    #Separating input and output\n",
        "    X = data.iloc[:, :total_cols_in_data]\n",
        "    X.drop(X.columns[the_class_column_index], axis=1, inplace=True )\n",
        "    y = data.iloc[:, the_class_column_index]\n",
        "\n",
        "    # Get the Count of Columns in the shuffled X and y\n",
        "    total_cols_in_X = int(len(X.axes[1]))\n",
        "    total_cols_in_y = int(1)\n",
        "\n",
        "    # Get the Count of Records in the shuffled X and y\n",
        "    total_rows_in_X = int(len(X.axes[0]))\n",
        "    total_rows_in_y = int(len(y.axes[0]))\n",
        "\n",
        "    if synthetic:\n",
        "        X = np.random.randn(total_rows_in_X,total_cols_in_X)\n",
        "        y = np.random.randn(total_rows_in_y,total_cols_in_y)\n",
        "\n",
        "        #  y = np.where(y[:,0]<0,1)\n",
        "        X,y = shuffle(X,y)\n",
        "\n",
        "\n",
        "    # Get the Count of Columns in the shuffled X and y\n",
        "    total_cols_in_X = int(len(X.axes[1]))\n",
        "    total_cols_in_y = int(1)\n",
        "\n",
        "    # Get the Count of Records in the shuffled X and y\n",
        "    total_rows_in_X = int(len(X.axes[0]))\n",
        "    total_rows_in_y = int(len(y.axes[0]))\n",
        "\n",
        "    print(X.shape[0],y.shape[0])\n",
        "    #Train data\n",
        "    train_X = X[:int(total_rows_in_X*0.8)]\n",
        "    train_y = y[:int(total_rows_in_y*0.8)]\n",
        "    #Test data\n",
        "    test_X = X[int(total_rows_in_X*0.8):]\n",
        "    test_y = y[int(total_rows_in_y*0.8):]\n",
        "\n",
        "    #Scaling\n",
        "    sc_X = StandardScaler()\n",
        "    train_X = sc_X.fit_transform(train_X)\n",
        "    test_X = sc_X.transform(test_X)\n",
        "\n",
        "    #converting y to one hot formats\n",
        "    train_y = pd.get_dummies(train_y).values\n",
        "    test_y = pd.get_dummies(test_y).values\n",
        "\n",
        "    #5 fold validation\n",
        "    folds = KFold(n_splits= 5, shuffle=True, random_state=SEED)\n",
        "\n",
        "\n",
        "    # Network Parameters\n",
        "    n_hidden_1 = 32 #number of neurons in 1st layer \n",
        "    n_hidden_2 = 16 #number of neurons in 2nd layer \n",
        "    n_input = total_cols_in_X #No of columns/Features in X\n",
        "    n_classes = y.nunique() #Output classes\n",
        "\n",
        "    if synthetic:\n",
        "        n_hidden_1 = 2048\n",
        "        n_hidden_2 = 1024\n",
        "        n_classes = 2\n",
        "        batch_size = 4096\n",
        "        pred_size = 8192\n",
        "    steps = len(train_X) # How many training data\n",
        "\n",
        "    #Resetting the graph\n",
        "    tf.compat.v1.get_default_graph() #tf.reset_default_graph()\n",
        "\n",
        "    #Defining Placeholders\n",
        "    X = tf.placeholder(\"float\", [None, n_input])\n",
        "    Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "    hold_prob1 = tf.placeholder(tf.float32)\n",
        "    hold_prob2 = tf.placeholder(tf.float32)\n",
        "\n",
        "    # Store layers weight & bias\n",
        "    weights = {\n",
        "        'h1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1],\\\n",
        "            mean = 0.0,stddev=0.2)),\n",
        "        'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2],\\\n",
        "            mean = 0.0,stddev = 0.2)),\n",
        "        'out': tf.Variable(tf.truncated_normal([n_hidden_2, n_classes],\\\n",
        "            mean = 0.0,stddev = 0.2))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.constant(0.1,shape = [n_hidden_1])),\n",
        "        'b2': tf.Variable(tf.constant(0.1,shape = [n_hidden_2])),\n",
        "        'out': tf.Variable(tf.constant(0.1,shape = [n_classes]))\n",
        "    }\n",
        "\n",
        "    # Create model\n",
        "    def multilayer_perceptron(x):\n",
        "        # First Hidden Layer\n",
        "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
        "        #Applying RELU nonlinearity\n",
        "        layer1_RELU = tf.nn.relu(layer_1)\n",
        "        #Applying Dropout\n",
        "        layer1_dropout = tf.nn.dropout(layer1_RELU,keep_prob=hold_prob1)\n",
        "        # Second hidden layer\n",
        "        layer_2 = tf.add(tf.matmul(layer1_dropout, weights['h2']), biases['b2'])\n",
        "        #Applying TANH nonlinearity\n",
        "        layer2_TANH = tf.nn.tanh(layer_2)\n",
        "        #Applying Dropout\n",
        "        layer2_dropout = tf.nn.dropout(layer2_TANH,keep_prob=hold_prob2)\n",
        "        # Output layer\n",
        "        out_layer = tf.matmul(layer2_dropout, weights['out']) + biases['out']\n",
        "        return out_layer\n",
        "\n",
        "\n",
        "    # Building model\n",
        "    logits = multilayer_perceptron(X)\n",
        "\n",
        "    # Defining Loss Function\n",
        "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "        logits=logits, labels=Y))\n",
        "    #Defining optimizer\n",
        "    optimizer = tf.train.RMSPropOptimizer(learning_rate=lr)\n",
        "    #Defining what to minimize\n",
        "    train_op = optimizer.minimize(loss_op)\n",
        "    # Initializing the variables\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    #Graph equations for Accuracy\n",
        "    matches = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
        "    acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
        "\n",
        "    remaining_train = len(train_X) % batch_size\n",
        "    remaining_test= len(test_X) % pred_size\n",
        "\n",
        "    # Create arrays and dataframes to store results\n",
        "    oof_preds = np.zeros(( train_X.shape[0] , n_classes ) )\n",
        "    sub_preds = np.zeros(( test_X.shape[0] , n_classes ) )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #Opening our Session\n",
        "    with tf.Session() as sess:\n",
        "        \n",
        "        #k-fold validation\n",
        "        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_X,train_y)):\n",
        "            print( \"Fold {} has started\".format(n_fold) )\n",
        "            X_train, y_train = train_X[train_idx], train_y[train_idx]\n",
        "            X_cv, y_cv = train_X[valid_idx], train_y[valid_idx]\n",
        "            #Running initialization of variables\n",
        "            sess.run(init)\n",
        "            # Writing down the for loop for epochs\n",
        "            for epoch in tqdm(range(training_epochs)):\n",
        "                #For loop for batches\n",
        "                for i in range(0,steps - remaining_train , batch_size):\n",
        "                    #Getting training data to be fed into the graphs\n",
        "                    batch_x, batch_y = X_train[i:i+batch_size],y_train[i:i+batch_size]\n",
        "                    # Training batch by batch\n",
        "                    _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,Y: batch_y,\\\n",
        "                        hold_prob1:0.7,hold_prob2:0.8})\n",
        "                \n",
        "                #Feeding last batch of train data\n",
        "                batch_x, batch_y = X_train[-remaining_train:],y_train[-remaining_train:]\n",
        "                _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,Y: batch_y,\\\n",
        "                    hold_prob1:0.7,hold_prob2:0.8})\n",
        "\n",
        "            #Calculating predictions over validation data\n",
        "            remaining_cv = len(X_cv) % pred_size\n",
        "            preds_on_cv = list()\n",
        "\n",
        "            for j in range(0,len(X_cv) - remaining_cv,pred_size):\n",
        "                batch_x = X_cv[j:j+pred_size]\n",
        "                preds_on_cv_temp = tf.nn.softmax(logits).eval(\n",
        "                    feed_dict ={X: batch_x,hold_prob1:1.0, hold_prob2:1.0})\n",
        "                preds_on_cv_temp = list(preds_on_cv_temp)\n",
        "                preds_on_cv.extend(preds_on_cv_temp)\n",
        "\n",
        "            #Calculating predictions over last batch of CV data\n",
        "            if remaining_cv != 0:\n",
        "                batch_x = X_cv[-remaining_cv:]\n",
        "                preds_on_cv_temp = tf.nn.softmax(logits).eval(\n",
        "                        feed_dict ={X: batch_x,hold_prob1:1.0, hold_prob2:1.0})\n",
        "                preds_on_cv_temp = list(preds_on_cv_temp)\n",
        "                preds_on_cv.extend(preds_on_cv_temp)\n",
        "\n",
        "            #Calculating loss and accuracy for CV\n",
        "            acc_on_cv,loss_on_cv = sess.run([acc,loss_op],feed_dict ={X: X_cv, Y: y_cv,\\\n",
        "                hold_prob1:1.0, hold_prob2:1.0})\n",
        "            #Calculating AUC on Cross Validation data\n",
        "            auc_on_cv = roc_auc_score(y_cv,preds_on_cv)\n",
        "            #Printing CV statistics after each epoch\n",
        "            print(\"Accuracy:\",acc_on_cv,\"Loss:\",loss_on_cv,\"AUC:\",auc_on_cv)\n",
        "            \n",
        "            #Calculating predictions over test data\n",
        "            preds_on_test = list()\n",
        "            for j in range(0,len(test_X) - remaining_test,pred_size):\n",
        "                batch_x = test_X[j:j+pred_size]\n",
        "                preds_on_test_temp = tf.nn.softmax(logits).eval(\n",
        "                    feed_dict ={X: batch_x,hold_prob1:1.0, hold_prob2:1.0})\n",
        "                preds_on_test_temp = list(preds_on_test_temp)\n",
        "                preds_on_test.extend(preds_on_test_temp)\n",
        "            #Calculating predictions over last batch of test data\n",
        "            if remaining_test != 0:\n",
        "                batch_x = test_X[-remaining_test:]\n",
        "                preds_on_test_temp = sess.run(tf.nn.softmax(logits),\\\n",
        "                        feed_dict ={X: batch_x,hold_prob1:1.0, hold_prob2:1.0})\n",
        "                preds_on_test_temp = list(preds_on_test_temp)\n",
        "                preds_on_test.extend(preds_on_test_temp)\n",
        "        oof_preds[valid_idx] = np.array(preds_on_cv)\n",
        "        sub_preds += np.array(preds_on_test) / folds.n_splits\n",
        "        print(\"Validation Results:\")\n",
        "        print(\"Accuracy:\",acc_on_cv,\"Loss:\",loss_on_cv,\"AUC:\",auc_on_cv)\n",
        "    print(\"Training and Scoring on Validation data done \\n\")\n",
        "\n",
        "\n",
        "    acc_on_test = accuracy_score(test_y.argmax(axis=1),sub_preds.argmax(axis=1))\n",
        "    loss_on_test = log_loss(test_y,sub_preds)\n",
        "    auc_on_test = roc_auc_score(test_y,sub_preds)\n",
        "    print(\"Test Results are below: \\n\")\n",
        "    print(\"Accuracy:\",acc_on_test,\" \\n Loss:\",loss_on_test,\" \\n AUC:\",auc_on_test)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUnmBLe3eMPc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "238f1f57-2993-4789-826f-d511736b4a1d"
      },
      "source": [
        "perceptron_prep_and_implementation(peakHours)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1288 1288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold 0 has started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:09<00:00,  5.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7330097 Loss: 0.48627508 AUC: 0.8290546662639686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/50 [00:00<00:08,  5.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold 1 has started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:09<00:00,  5.49it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.74757284 Loss: 0.47605097 AUC: 0.8583792289535799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/50 [00:00<00:09,  5.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold 2 has started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:09<00:00,  5.44it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.80582523 Loss: 0.40789112 AUC: 0.8816017316017317\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/50 [00:00<00:08,  5.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold 3 has started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:09<00:00,  5.27it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.776699 Loss: 0.47490352 AUC: 0.8326364436619718\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/50 [00:00<00:09,  5.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold 4 has started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:09<00:00,  5.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7815534 Loss: 0.4550592 AUC: 0.8558823529411765\n",
            "Validation Results:\n",
            "Accuracy: 0.7815534 Loss: 0.4550592 AUC: 0.8558823529411765\n",
            "Training and Scoring on Validation data done \n",
            "\n",
            "Test Results are below: \n",
            "\n",
            "Accuracy: 0.7674418604651163  \n",
            " Loss: 0.4977352981955849  \n",
            " AUC: 0.8543294581030432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlYEPUa26kjY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "ca5e4f7c-0350-4168-d85e-181a285d8a89"
      },
      "source": [
        "perceptron_prep_and_implementation(offPeak)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "920 920\n",
            "Fold 0 has started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:07<00:00,  7.13it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6824324 Loss: 0.563671 AUC: 0.7735294117647059\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/50 [00:00<00:06,  7.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold 1 has started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:07<00:00,  7.12it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.68707484 Loss: 0.57850456 AUC: 0.7582831325301204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/50 [00:00<00:06,  7.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold 2 has started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:07<00:00,  6.93it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.71428573 Loss: 0.5410436 AUC: 0.8039638932496075\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/50 [00:00<00:07,  6.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold 3 has started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:07<00:00,  7.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6258503 Loss: 0.64469004 AUC: 0.7013059701492538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/50 [00:00<00:06,  7.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold 4 has started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:07<00:00,  6.86it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.64625853 Loss: 0.64696395 AUC: 0.7204738985560903\n",
            "Validation Results:\n",
            "Accuracy: 0.64625853 Loss: 0.64696395 AUC: 0.7204738985560903\n",
            "Training and Scoring on Validation data done \n",
            "\n",
            "Test Results are below: \n",
            "\n",
            "Accuracy: 0.7771739130434783  \n",
            " Loss: 0.4526399985628608  \n",
            " AUC: 0.8737379807692307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isVYITt_p5XR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}